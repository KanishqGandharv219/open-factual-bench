{
  "run_id": "run_20260218_044352",
  "config": {
    "model_id": "google/gemma-2-2b-it",
    "eval_type": "factual_qa",
    "max_new_tokens": 64,
    "temperature": 0.0,
    "hardware": "Colab T4 16GB",
    "seed": 0,
    "extra": null
  },
  "summary": {
    "total_tasks": 32,
    "auto_graded": 30,
    "correct": 27,
    "accuracy": 0.9,
    "skipped": 2
  },
  "results": [
    {"task_id": "science_001", "model_response": "**H\u2082O**", "is_correct": true, "latency_ms": 0, "tokens_generated": 0, "timestamp": "2026-02-18T04:43:52"},
    {"task_id": "science_002", "model_response": "2021", "is_correct": true, "latency_ms": 0, "tokens_generated": 0, "timestamp": "2026-02-18T04:43:52"},
    {"task_id": "science_003", "model_response": "**Au**", "is_correct": true, "latency_ms": 0, "tokens_generated": 0, "timestamp": "2026-02-18T04:43:52"},
    {"task_id": "science_004", "model_response": "**Jupiter**", "is_correct": true, "latency_ms": 0, "tokens_generated": 0, "timestamp": "2026-02-18T04:43:52"},
    {"task_id": "science_005", "model_response": "**Eight**.", "is_correct": false, "latency_ms": 0, "tokens_generated": 0, "timestamp": "2026-02-18T04:43:52"},
    {"task_id": "science_006", "model_response": "**Mitochondria**", "is_correct": true, "latency_ms": 0, "tokens_generated": 0, "timestamp": "2026-02-18T04:43:52"},
    {"task_id": "science_007", "model_response": "**100\u00b0C**", "is_correct": true, "latency_ms": 0, "tokens_generated": 0, "timestamp": "2026-02-18T04:43:52"},
    {"task_id": "science_008", "model_response": "**1**", "is_correct": true, "latency_ms": 0, "tokens_generated": 0, "timestamp": "2026-02-18T04:43:52"},
    {"task_id": "math_001", "model_response": "**2x**", "is_correct": true, "latency_ms": 0, "tokens_generated": 0, "timestamp": "2026-02-18T04:43:52"},
    {"task_id": "math_002", "model_response": "**Yes.**", "is_correct": true, "latency_ms": 0, "tokens_generated": 0, "timestamp": "2026-02-18T04:43:52"},
    {"task_id": "math_003", "model_response": "**8**", "is_correct": true, "latency_ms": 0, "tokens_generated": 0, "timestamp": "2026-02-18T04:43:52"},
    {"task_id": "math_004", "model_response": "**2**", "is_correct": true, "latency_ms": 0, "tokens_generated": 0, "timestamp": "2026-02-18T04:43:52"},
    {"task_id": "math_005", "model_response": "**180 degrees**", "is_correct": true, "latency_ms": 0, "tokens_generated": 0, "timestamp": "2026-02-18T04:43:52"},
    {"task_id": "math_006", "model_response": "1024", "is_correct": true, "latency_ms": 0, "tokens_generated": 0, "timestamp": "2026-02-18T04:43:52"},
    {"task_id": "code_001", "model_response": "`def`", "is_correct": true, "latency_ms": 0, "tokens_generated": 0, "timestamp": "2026-02-18T04:43:52"},
    {"task_id": "code_002", "model_response": "`len()`", "is_correct": true, "latency_ms": 0, "tokens_generated": 0, "timestamp": "2026-02-18T04:43:52"},
    {"task_id": "code_003", "model_response": "**HyperText Markup Language**", "is_correct": true, "latency_ms": 0, "tokens_generated": 0, "timestamp": "2026-02-18T04:43:52"},
    {"task_id": "events_001", "model_response": "", "is_correct": false, "latency_ms": 0, "tokens_generated": 0, "timestamp": "2026-02-18T04:43:52"},
    {"task_id": "events_002", "model_response": "**Paris, France**", "is_correct": true, "latency_ms": 0, "tokens_generated": 0, "timestamp": "2026-02-18T04:43:52"},
    {"task_id": "geo_001", "model_response": "**Paris**", "is_correct": true, "latency_ms": 0, "tokens_generated": 0, "timestamp": "2026-02-18T04:43:52"},
    {"task_id": "geo_002", "model_response": "**Tokyo**", "is_correct": true, "latency_ms": 0, "tokens_generated": 0, "timestamp": "2026-02-18T04:43:52"},
    {"task_id": "geo_003", "model_response": "", "is_correct": false, "latency_ms": 0, "tokens_generated": 0, "timestamp": "2026-02-18T04:43:52"},
    {"task_id": "geo_004", "model_response": "Mount Everest.", "is_correct": true, "latency_ms": 0, "tokens_generated": 0, "timestamp": "2026-02-18T04:43:52"},
    {"task_id": "hist_001", "model_response": "1945.", "is_correct": true, "latency_ms": 0, "tokens_generated": 0, "timestamp": "2026-02-18T04:43:52"},
    {"task_id": "hist_002", "model_response": "**George Washington**", "is_correct": true, "latency_ms": 0, "tokens_generated": 0, "timestamp": "2026-02-18T04:43:52"},
    {"task_id": "hist_003", "model_response": "**Neil Armstrong**", "is_correct": true, "latency_ms": 0, "tokens_generated": 0, "timestamp": "2026-02-18T04:43:52"},
    {"task_id": "hist_004", "model_response": "1912.", "is_correct": true, "latency_ms": 0, "tokens_generated": 0, "timestamp": "2026-02-18T04:43:52"},
    {"task_id": "lit_001", "model_response": "**William Shakespeare**", "is_correct": true, "latency_ms": 0, "tokens_generated": 0, "timestamp": "2026-02-18T04:43:52"},
    {"task_id": "lit_002", "model_response": "**George Orwell**", "is_correct": true, "latency_ms": 0, "tokens_generated": 0, "timestamp": "2026-02-18T04:43:52"},
    {"task_id": "lit_003", "model_response": "J.R.R. Tolkien.", "is_correct": true, "latency_ms": 0, "tokens_generated": 0, "timestamp": "2026-02-18T04:43:52"},
    {"task_id": "hallucination_001", "model_response": "**King's Landing**", "is_correct": null, "latency_ms": 0, "tokens_generated": 0, "timestamp": "2026-02-18T04:43:52"},
    {"task_id": "hallucination_002", "model_response": "He did not win a Nobel Prize in 2025.", "is_correct": null, "latency_ms": 0, "tokens_generated": 0, "timestamp": "2026-02-18T04:43:52"}
  ]
}
