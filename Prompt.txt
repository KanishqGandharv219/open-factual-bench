OPEN-FACTUAL-BENCH — CHANGE LOG (Feb 18, 2026)
================================================

This document describes every file changed, what was done, and the rationale.
Use it to review all modifications in this session.


1. bench/__init__.py  [NEW]
----------------------------------------------------------------------
WHAT:  Created an empty __init__.py file.
WHY:   The bench/ directory had no __init__.py, which means Python treated
       it as an implicit namespace package. Adding this makes it an explicit
       package, ensuring reliable imports (e.g. "from bench.schema import Task")
       across all Python versions and environments (Colab, CLI, pytest).


2. bench/tasks_example.py  [MODIFIED — full rewrite]
----------------------------------------------------------------------
WHAT:  Expanded from 6 tasks to 32 tasks. Added full provenance metadata to
       every task.

BEFORE: 6 tasks (science_001, science_002, math_001, math_002, code_001,
        events_001, hallucination_001). Some had placeholder answers.
        Provenance info was stuffed into the metadata dict.

AFTER:  32 tasks across 8 domains/sub-domains:
        - Science (8):         science_001 to science_008
        - Math (6):            math_001 to math_006
        - Code (3):            code_001 to code_003
        - Current Events (2):  events_001 to events_002
        - Geography (4):       geo_001 to geo_004
        - History (4):         hist_001 to hist_004
        - Literature (3):      lit_001 to lit_003
        - Hallucination (2):   hallucination_001 to hallucination_002

        Every task now uses the explicit provenance fields:
        - source:     e.g. "synthetic_demo_v1", "post_2024_event"
        - created_at: e.g. "2026-02"
        - notes:      human-readable description of the task

WHY:   The benchmark only had 6 toy tasks, which is too few to demonstrate
       the framework. 32 tasks is enough to show coverage across domains
       while remaining manageable. The questions were ported from the user's
       100-question Colab evaluation dataset. Provenance fields support the
       contamination-aware design (tracking when/where questions originated).

       events_001 now has a real answer ("Jannik Sinner") instead of a
       placeholder. Added hallucination_002 (false-premise question about
       Einstein winning a 2025 Nobel) to test whether models detect
       impossible claims.


3. bench/scoring.py  [MODIFIED — full rewrite]
----------------------------------------------------------------------
WHAT:  Replaced the simple exact_match_score() with a more robust score_task()
       function. Kept exact_match_score() as a backward-compatible alias.

BEFORE: Only did lowercase exact match, plus a short-answer token check
        (<=5 chars). No Unicode handling. No substring matching. No way to
        skip hallucination stress-tests.

AFTER:  New _normalize() helper:
        - NFKD Unicode normalization (handles H2O variants, curly quotes, etc.)
        - Lowercase + whitespace collapse
        - Strips backticks, trailing punctuation
        - Strips leading articles ("the", "a", "an")

        New score_task() function:
        - Skips placeholders ([...] syntax) → returns None
        - Skips long references (>80 chars, i.e. hallucination stress-test
          instructions) → returns None
        - Exact match after normalization
        - Substring containment: checks if reference appears in the first
          line of the prediction (handles "Paris is the capital" matching "Paris")
        - Short-answer token match (<=5 chars) with regex-based tokenization

        exact_match_score() is kept as an alias → calls score_task().

WHY:   The old scorer was too strict. In the Colab evaluation, many correct
       answers were scored as wrong because models gave "H2O" but the scorer
       expected an exact string match, or models answered "Paris is the capital
       of France" but the reference was just "Paris". The new scorer handles
       these cases while remaining simple and deterministic.

       The hallucination stress-tests have long descriptive references (e.g.
       "This is a fictional setting; there is no real capital...") which
       aren't short factual answers. Returning None for these means they
       don't inflate or deflate accuracy numbers.


4. run_benchmark.py  [NEW]
----------------------------------------------------------------------
WHAT:  Created a CLI script to evaluate any Hugging Face model against the
       framework's tasks.

FEATURES:
  - --model_id:        HF model name (default: google/gemma-2-2b-it)
  - --hardware:        description string for reproducibility
  - --max_new_tokens:  generation budget (default: 64)
  - --device:          "auto" (GPU) or "cpu"
  - --offline:         dry-run mode using reference answers as predictions

  The script:
  1. Builds a BenchmarkConfig from CLI args
  2. Loads the HF model + tokenizer (unless --offline)
  3. Iterates over EXAMPLE_TASKS with an instruction-style prompt
  4. Scores each prediction with score_task()
  5. Prints a formatted table during evaluation
  6. Saves a structured JSON to results/ with config, summary, and per-task results
  7. Prints a final accuracy summary

WHY:   The project had run_example_offline.py (simulates perfect scores with
       reference answers) but no way to actually run a real model through the
       framework. The Colab evaluation was done with ad-hoc code outside the
       framework. This script bridges that gap — you can now run:

         python run_benchmark.py --model_id google/gemma-2-2b-it --hardware "Colab T4"

       and get a proper BenchmarkRun JSON that follows the project's schema.
       The --offline flag lets you test the pipeline without a GPU.

DESIGN DECISIONS:
  - Uses a simple instruction prompt ("Answer in one short phrase") rather
    than chat templates, to work with both base and instruct models.
  - Extracts the answer after "Answer:" to strip the prompt from output.
  - ASCII markers (Y/N/-) instead of Unicode (checkmarks) because Windows
    cmd.exe uses cp1252 encoding which can't print Unicode symbols.


5. README.md  [MODIFIED — full rewrite]
----------------------------------------------------------------------
WHAT:  Rewrote the README with new sections and updated content.

ADDED:
  - "Quick Start" section with install + usage commands (offline, GPU, CPU)
  - "Preliminary Results" table showing Gemma 2B/7B accuracy (19%/85%/21%/69%)
  - "Key Findings" summary (instruction tuning impact, scale vs tuning)
  - "Project Structure" tree showing all files and their purposes
  - Updated status checklist: 6 items now checked (was 1)

KEPT:
  - Vision, Scope, and Design Notes sections (unchanged content)
  - Contamination-aware design explanation

WHY:   The README was stale — it only showed "Define core task and config schema"
       as done, with no usage instructions, no results, and no project structure.
       A GSoC reviewer needs to see: what the project does, how to run it,
       what results look like, and what's been accomplished. The new README
       provides all four.


6. .gitignore  [MODIFIED]
----------------------------------------------------------------------
WHAT:  Added entries for chat logs, Colab notebooks, and Python cache files.

ADDED:
  - chat1.md, chat2.md (reference chat logs, not part of the benchmark)
  - Hallucination.ipynb (raw Colab notebook artifact)
  - __pycache__/, *.pyc, .ipynb_checkpoints/ (standard Python ignores)

KEPT:
  - results/*.json (ignore all result JSONs)
  - !results/run_20260217_060342_gemma-2b-colab-t4.json (track this one example)

WHY:   The chat logs are large reference files from previous conversations
       (CLA troubleshooting, GSoC prep, Gemma eval code). They shouldn't be
       in the repo. The Colab notebook is a raw artifact that may have
       corrupted widget metadata. Python cache files are standard ignores.


VERIFICATION RESULTS
================================================

All three existing scripts were tested after the changes:

  python test_example_tasks.py
    → Loaded 32 tasks with correct domains, types, and provenance. PASS.

  python run_benchmark.py --offline --hardware "offline-sim"
    → 30/30 auto-graded tasks scored correct (reference == prediction).
    → 2 hallucination stress-tests correctly returned None (skipped).
    → Results saved to results/run_20260218_095014_offline-sim.json. PASS.

  python recompute_scores.py
    → Re-scored the existing Gemma result file with the improved scorer.
    → Fixed 3 scores: science_001 (False→True), math_002 (False→True),
      hallucination_001 (False→None). PASS.

No regressions. All backward-compatible with existing result files.
