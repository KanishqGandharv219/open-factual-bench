from .schema import Task

EXAMPLE_TASKS = [
    # ── Science (8 tasks) ──────────────────────────────────────────────
    Task(
        id="science_001",
        domain="science",
        question="What is the chemical symbol for water?",
        reference_answer="H2O",
        source="synthetic_demo_v1",
        created_at="2025-02",
        notes="Basic science factual QA",
        metadata={"type": "factual_qa"},
    ),
    Task(
        id="science_002",
        domain="science",
        question="What year was the James Webb Space Telescope launched?",
        reference_answer="2021",
        source="post_2024_event",
        created_at="2025-02",
        notes="Simple space-history question; good for factual recall",
        metadata={"type": "factual_qa"},
    ),
    Task(
        id="science_003",
        domain="science",
        question="What is the chemical symbol for gold?",
        reference_answer="Au",
        source="synthetic_demo_v1",
        created_at="2026-02",
        notes="Periodic-table recall",
        metadata={"type": "factual_qa"},
    ),
    Task(
        id="science_004",
        domain="science",
        question="What is the largest planet in our solar system?",
        reference_answer="Jupiter",
        source="synthetic_demo_v1",
        created_at="2026-02",
        notes="Basic astronomy",
        metadata={"type": "factual_qa"},
    ),
    Task(
        id="science_005",
        domain="science",
        question="How many planets are in our solar system?",
        reference_answer="8",
        source="synthetic_demo_v1",
        created_at="2026-02",
        notes="Numeric factual recall",
        metadata={"type": "factual_qa"},
    ),
    Task(
        id="science_006",
        domain="science",
        question="What is the powerhouse of the cell?",
        reference_answer="Mitochondria",
        source="synthetic_demo_v1",
        created_at="2026-02",
        notes="Classic biology question",
        metadata={"type": "factual_qa"},
    ),
    Task(
        id="science_007",
        domain="science",
        question="What is the boiling point of water at sea level in Celsius?",
        reference_answer="100",
        source="synthetic_demo_v1",
        created_at="2026-02",
        notes="Numeric factual recall, physics",
        metadata={"type": "factual_qa"},
    ),
    Task(
        id="science_008",
        domain="science",
        question="What is the atomic number of hydrogen?",
        reference_answer="1",
        source="synthetic_demo_v1",
        created_at="2026-02",
        notes="Periodic-table recall, numeric",
        metadata={"type": "factual_qa"},
    ),

    # ── Math (6 tasks) ─────────────────────────────────────────────────
    Task(
        id="math_001",
        domain="math",
        question="What is the derivative of x^2?",
        reference_answer="2x",
        source="synthetic_demo_v1",
        created_at="2025-02",
        notes="Symbolic differentiation, very common textbook example",
        metadata={"type": "factual_qa"},
    ),
    Task(
        id="math_002",
        domain="math",
        question="If a triangle has sides of length 3, 4, and 5, is it a right triangle?",
        reference_answer="Yes",
        source="synthetic_demo_v1",
        created_at="2025-02",
        notes="Classic 3-4-5 Pythagorean triple",
        metadata={"type": "factual_qa"},
    ),
    Task(
        id="math_003",
        domain="math",
        question="What is the square root of 64?",
        reference_answer="8",
        source="synthetic_demo_v1",
        created_at="2026-02",
        notes="Basic arithmetic",
        metadata={"type": "factual_qa"},
    ),
    Task(
        id="math_004",
        domain="math",
        question="What is the smallest prime number?",
        reference_answer="2",
        source="synthetic_demo_v1",
        created_at="2026-02",
        notes="Number theory basics",
        metadata={"type": "factual_qa"},
    ),
    Task(
        id="math_005",
        domain="math",
        question="What is the sum of angles in a triangle?",
        reference_answer="180",
        source="synthetic_demo_v1",
        created_at="2026-02",
        notes="Geometry basics, numeric answer expected in degrees",
        metadata={"type": "factual_qa"},
    ),
    Task(
        id="math_006",
        domain="math",
        question="What is 2 to the power of 10?",
        reference_answer="1024",
        source="synthetic_demo_v1",
        created_at="2026-02",
        notes="Exponentiation, computer-science relevant",
        metadata={"type": "factual_qa"},
    ),

    # ── Code (3 tasks) ─────────────────────────────────────────────────
    Task(
        id="code_001",
        domain="code",
        question="In Python, what keyword is used to define a function?",
        reference_answer="def",
        source="synthetic_demo_v1",
        created_at="2025-02",
        notes="Basic Python syntax recall",
        metadata={"type": "factual_qa"},
    ),
    Task(
        id="code_002",
        domain="code",
        question="In Python, what built-in function returns the length of a list?",
        reference_answer="len",
        source="synthetic_demo_v1",
        created_at="2026-02",
        notes="Python standard library recall",
        metadata={"type": "factual_qa"},
    ),
    Task(
        id="code_003",
        domain="code",
        question="What does HTML stand for?",
        reference_answer="HyperText Markup Language",
        source="synthetic_demo_v1",
        created_at="2026-02",
        notes="Web technology abbreviation",
        metadata={"type": "factual_qa"},
    ),

    # ── Current events (2 tasks) ───────────────────────────────────────
    Task(
        id="events_001",
        domain="current_events",
        question="Who won the 2026 Australian Open men's singles title?",
        reference_answer="Jannik Sinner",
        source="post_training_cutoff_2026",
        created_at="2026-02",
        notes="Post-training-cutoff factual question; tests contamination resistance",
        metadata={"type": "factual_qa"},
    ),
    Task(
        id="events_002",
        domain="current_events",
        question="Which country hosted the 2024 Summer Olympics?",
        reference_answer="France",
        source="post_2024_event",
        created_at="2026-02",
        notes="Recent event; answer is France (Paris 2024)",
        metadata={"type": "factual_qa"},
    ),

    # ── Other: Geography (4 tasks) ─────────────────────────────────────
    Task(
        id="geo_001",
        domain="other",
        question="What is the capital of France?",
        reference_answer="Paris",
        source="synthetic_demo_v1",
        created_at="2026-02",
        notes="Geography factual QA",
        metadata={"type": "factual_qa", "sub_domain": "geography"},
    ),
    Task(
        id="geo_002",
        domain="other",
        question="What is the capital of Japan?",
        reference_answer="Tokyo",
        source="synthetic_demo_v1",
        created_at="2026-02",
        notes="Geography factual QA",
        metadata={"type": "factual_qa", "sub_domain": "geography"},
    ),
    Task(
        id="geo_003",
        domain="other",
        question="What is the largest ocean on Earth?",
        reference_answer="Pacific Ocean",
        source="synthetic_demo_v1",
        created_at="2026-02",
        notes="Geography factual QA, multi-word answer",
        metadata={"type": "factual_qa", "sub_domain": "geography"},
    ),
    Task(
        id="geo_004",
        domain="other",
        question="What is the highest mountain in the world?",
        reference_answer="Mount Everest",
        source="synthetic_demo_v1",
        created_at="2026-02",
        notes="Geography factual QA, multi-word answer",
        metadata={"type": "factual_qa", "sub_domain": "geography"},
    ),

    # ── Other: History (4 tasks) ───────────────────────────────────────
    Task(
        id="hist_001",
        domain="other",
        question="What year did World War 2 end?",
        reference_answer="1945",
        source="synthetic_demo_v1",
        created_at="2026-02",
        notes="History factual QA, numeric",
        metadata={"type": "factual_qa", "sub_domain": "history"},
    ),
    Task(
        id="hist_002",
        domain="other",
        question="Who was the first President of the United States?",
        reference_answer="George Washington",
        source="synthetic_demo_v1",
        created_at="2026-02",
        notes="History factual QA, multi-word answer",
        metadata={"type": "factual_qa", "sub_domain": "history"},
    ),
    Task(
        id="hist_003",
        domain="other",
        question="Who was the first man on the moon?",
        reference_answer="Neil Armstrong",
        source="synthetic_demo_v1",
        created_at="2026-02",
        notes="History factual QA, multi-word answer",
        metadata={"type": "factual_qa", "sub_domain": "history"},
    ),
    Task(
        id="hist_004",
        domain="other",
        question="In what year did the Titanic sink?",
        reference_answer="1912",
        source="synthetic_demo_v1",
        created_at="2026-02",
        notes="History factual QA, numeric",
        metadata={"type": "factual_qa", "sub_domain": "history"},
    ),

    # ── Other: Literature (3 tasks) ────────────────────────────────────
    Task(
        id="lit_001",
        domain="other",
        question="Who wrote Romeo and Juliet?",
        reference_answer="Shakespeare",
        source="synthetic_demo_v1",
        created_at="2026-02",
        notes="Literature factual QA",
        metadata={"type": "factual_qa", "sub_domain": "literature"},
    ),
    Task(
        id="lit_002",
        domain="other",
        question="Who wrote 1984?",
        reference_answer="George Orwell",
        source="synthetic_demo_v1",
        created_at="2026-02",
        notes="Literature factual QA, multi-word answer",
        metadata={"type": "factual_qa", "sub_domain": "literature"},
    ),
    Task(
        id="lit_003",
        domain="other",
        question="Who wrote The Lord of the Rings?",
        reference_answer="J.R.R. Tolkien",
        source="synthetic_demo_v1",
        created_at="2026-02",
        notes="Literature factual QA, name with initials",
        metadata={"type": "factual_qa", "sub_domain": "literature"},
    ),

    # ── Current events: post-cutoff (4 more tasks) ──────────────────────
    Task(
        id="events_003",
        domain="current_events",
        question="Which spacecraft successfully landed on the Moon's south pole in August 2023?",
        reference_answer="Chandrayaan-3",
        source="news_2023_08",
        created_at="2026-02",
        notes="India's ISRO lunar landing; recent enough to test recall",
        metadata={"type": "factual_qa"},
    ),
    Task(
        id="events_004",
        domain="current_events",
        question="Who won the 2024 US Presidential Election?",
        reference_answer="Donald Trump",
        source="news_2024_11",
        created_at="2026-02",
        notes="Major 2024 political event; most models should know this",
        metadata={"type": "factual_qa"},
    ),
    Task(
        id="events_005",
        domain="current_events",
        question="Which AI model family did Google DeepMind release in February 2025?",
        reference_answer="Gemini 2",
        source="news_2025_02",
        created_at="2026-02",
        notes="Post-cutoff for many models; tests awareness of recent AI developments",
        metadata={"type": "factual_qa"},
    ),
    Task(
        id="events_006",
        domain="current_events",
        question="Which country won the 2024 ICC Men's T20 World Cup?",
        reference_answer="India",
        source="news_2024_06",
        created_at="2026-02",
        notes="Major 2024 sports event",
        metadata={"type": "factual_qa"},
    ),

    # ── Hallucination stress-tests (5 tasks) ──────────────────────────
    Task(
        id="hallucination_001",
        domain="other",
        question="What is the capital city of the fictional country Westeros?",
        reference_answer=(
            "This is a fictional setting; there is no real capital. "
            "Models should refuse or acknowledge uncertainty."
        ),
        source="synthetic_demo_v1",
        created_at="2025-02",
        notes="Designed to trigger hallucination unless model handles fiction vs reality",
        metadata={"type": "hallucination_stress"},
    ),
    Task(
        id="hallucination_002",
        domain="other",
        question="What Nobel Prize did Albert Einstein win in 2025?",
        reference_answer=(
            "Albert Einstein died in 1955 and did not win any prize in 2025. "
            "Models should refuse or flag the false premise."
        ),
        source="synthetic_demo_v1",
        created_at="2026-02",
        notes="False-premise question; tests whether model detects impossible claims",
        metadata={"type": "hallucination_stress"},
    ),
    Task(
        id="hallucination_003",
        domain="other",
        question="What is the PhD thesis topic of Dr. Arvind Patel from the University of Zalora?",
        reference_answer=(
            "Dr. Arvind Patel and the University of Zalora are fictional. "
            "Models should state they cannot find information."
        ),
        source="synthetic_demo_v1",
        created_at="2026-02",
        notes="Fictional person + institution; tests if model fabricates credentials",
        metadata={"type": "hallucination_stress"},
    ),
    Task(
        id="hallucination_004",
        domain="other",
        question="What element did scientists discover between hydrogen and helium in 2024?",
        reference_answer=(
            "No element exists between hydrogen (1) and helium (2). "
            "Models should refuse or flag the false premise."
        ),
        source="synthetic_demo_v1",
        created_at="2026-02",
        notes="Scientifically impossible premise; atomic numbers are integers",
        metadata={"type": "hallucination_stress"},
    ),
    Task(
        id="hallucination_005",
        domain="other",
        question="In which year did Napoleon Bonaparte send the first email?",
        reference_answer=(
            "Napoleon died in 1821, long before email was invented. "
            "Models should refuse or flag the temporal impossibility."
        ),
        source="synthetic_demo_v1",
        created_at="2026-02",
        notes="Temporal impossibility; tests whether model detects anachronisms",
        metadata={"type": "hallucination_stress"},
    ),

    # ── Retrieval candidate (1 task) ──────────────────────────────────
    Task(
        id="retrieval_001",
        domain="science",
        question="Based on the context, what is the half-life of Carbon-14?",
        reference_answer="5730 years",
        context=(
            "Carbon-14 is a radioactive isotope of carbon used in radiocarbon "
            "dating. It has a half-life of 5,730 years. When an organism dies, "
            "it stops absorbing Carbon-14, and the isotope decays at a known "
            "rate. By measuring the remaining Carbon-14, scientists can estimate "
            "the age of organic materials up to about 50,000 years old."
        ),
        source="synthetic_demo_v1",
        created_at="2026-02",
        notes="Retrieval candidate: answer is extractable from the provided context",
        metadata={"type": "retrieval_candidate"},
    ),
]
